{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/WildlifeDatasets/wildlife-datasets@develop --quiet\n",
    "!pip install git+https://github.com/WildlifeDatasets/wildlife-tools --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from wildlife_tools.similarity.pairwise.collectors import CollectCounts\n",
    "from wildlife_tools.features import SuperPointExtractor, AlikedExtractor, DiskExtractor, SiftExtractor, DeepFeatures\n",
    "from wildlife_tools.similarity import MatchLightGlue\n",
    "from wildlife_tools.similarity.calibration import IsotonicCalibration, LogisticCalibration\n",
    "from wildlife_tools.similarity.wildfusion import SimilarityPipeline # WildFusion\n",
    "from wildlife_tools.similarity.cosine import CosineSimilarity\n",
    "from wildlife_datasets.datasets import WildlifeDataset, AnimalCLEF2025\n",
    "from wildlife_datasets import splits\n",
    "\n",
    "from wildlife_tools.similarity.pairwise.base import MatchPairs, PairDataset\n",
    "from wildlife_tools.data import ImageDataset, FeatureDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import kornia.feature as KF\n",
    "import kornia as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# фикс сидов, чтобы обучение было воспроизводимым.\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "def BAKS(\n",
    "    y_true: List,\n",
    "    y_pred: List,\n",
    "    identity_test_only: List,\n",
    ") -> float:\n",
    "    \"\"\"Computes BAKS (balanced accuracy on known samples).\n",
    "\n",
    "    It ignores `identity_test_only` because they are unknown identities.\n",
    "\n",
    "    Args:\n",
    "        y_true (List): List of true labels.\n",
    "        y_score (List): List of scores.\n",
    "        identity_test_only (List): List of new identities (only in the testing set).\n",
    "\n",
    "    Returns:\n",
    "        Computed BAKS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Need to keep the object type due to mixed arrays\n",
    "    y_true = np.array(y_true, dtype=object)\n",
    "    y_pred = np.array(y_pred, dtype=object)\n",
    "    identity_test_only = np.array(identity_test_only, dtype=object)\n",
    "\n",
    "    # Remove data in identity_test_only\n",
    "    idx = np.where(~np.isin(y_true, identity_test_only))[0]\n",
    "    y_true_idx = y_true[idx]\n",
    "    y_pred_idx = y_pred[idx]\n",
    "    if len(y_true_idx) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    df = pd.DataFrame({\"y_true\": y_true_idx, \"y_pred\": y_pred_idx})\n",
    "\n",
    "    # Compute the balanced accuracy\n",
    "    accuracy = 0\n",
    "    for _, df_identity in df.groupby(\"y_true\"):\n",
    "        accuracy += (\n",
    "            1\n",
    "            / df[\"y_true\"].nunique()\n",
    "            * np.mean(df_identity[\"y_pred\"] == df_identity[\"y_true\"])\n",
    "        )\n",
    "    return accuracy\n",
    "\n",
    "def BAUS(\n",
    "    y_true: List, y_pred: List, identity_test_only: List, new_class: Union[int, str]\n",
    ") -> float:\n",
    "    \"\"\"Computes BAUS (balanced accuracy on unknown samples).\n",
    "\n",
    "    It handles only `identity_test_only` because they are unknown identities.\n",
    "\n",
    "    Args:\n",
    "        y_true (List): List of true labels.\n",
    "        y_score (List): List of scores.\n",
    "        identity_test_only (List): List of new identities (only in the testing set).\n",
    "        new_class (Union[int, str]): Name of the new class.\n",
    "\n",
    "    Returns:\n",
    "        Computed BAUS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Need to keep the object type due to mixed arrays\n",
    "    y_true = np.array(y_true, dtype=object)\n",
    "    y_pred = np.array(y_pred, dtype=object)\n",
    "    identity_test_only = np.array(identity_test_only, dtype=object)\n",
    "\n",
    "    # Remove data not in identity_test_only\n",
    "    idx = np.where(np.isin(y_true, identity_test_only))[0]\n",
    "    y_true_idx = y_true[idx]\n",
    "    y_pred_idx = y_pred[idx]\n",
    "    if len(y_true_idx) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    df = pd.DataFrame({\"y_true\": y_true_idx, \"y_pred\": y_pred_idx})\n",
    "\n",
    "    # Compute the balanced accuracy\n",
    "    accuracy = 0\n",
    "    for _, df_identity in df.groupby(\"y_true\"):\n",
    "        accuracy += (\n",
    "            1 / df[\"y_true\"].nunique() * np.mean(df_identity[\"y_pred\"] == new_class)\n",
    "        )\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function\n",
    "def split_data(dataset, data):\n",
    "    splitter = splits.OpenSetSplit(0.8, 0.1)\n",
    "    idx_train, idx_test = next(iter(splitter.split(data)))\n",
    "    training_dataloader = dataset.get_subset(idx_train)\n",
    "    val_dataloader = dataset.get_subset(idx_test)\n",
    "    return training_dataloader, val_dataloader\n",
    "\n",
    "def get_preds(labels, similarity, query_size):\n",
    "    pred_idx = similarity.argsort(axis=1)[:,-1]\n",
    "    pred_scores = similarity[range(query_size), pred_idx]\n",
    "    predictions = labels[pred_idx]\n",
    "    return [predictions, pred_scores]\n",
    "\n",
    "def to_torch(x: Union[List, np.array, torch.Tensor]):\n",
    "    if type(x) is List:\n",
    "        x_out = torch.tensor(x)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        x_out = x\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        x_out = torch.from_numpy(x)\n",
    "    else:\n",
    "        raise TypeError('img should be List, np.array or torch.Tensor')\n",
    "    return x_out\n",
    "\n",
    "def get_hits(dataset0, dataset1):\n",
    "    gt0 = dataset0.labels_string\n",
    "    gt1 = dataset1.labels_string\n",
    "    gt_grid0 = np.tile(gt0, (len(gt1), 1)).T\n",
    "    gt_grid1 = np.tile(gt1, (len(gt0), 1))\n",
    "    return gt_grid0 == gt_grid1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# matchers_func\n",
    "def fast_local_matchers(extractors, device='cpu'):\n",
    "    matchers = []\n",
    "    for extractor in extractors:\n",
    "        model_ = SimilarityPipeline(\n",
    "            matcher=MatchLightGlue(features=extractor, device=device),\n",
    "            extractor= extractors[extractor] ,\n",
    "            transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "            calibration=IsotonicCalibration()\n",
    "        )\n",
    "        matchers.append(model_)\n",
    "    return matchers\n",
    "\n",
    "def fast_priority_matcher(parallel=False, loaded_model=None, batch_size=64, device='cpu'):\n",
    "    model = timm.create_model('hf-hub:BVRA/wildlife-mega-L-384', num_classes=0, pretrained=True, device=device)\n",
    "    if loaded_model:\n",
    "        model.load_state_dict({i[7:]: value for i, value in torch.load(loaded_model)['model'].items()})\n",
    "        model.eval()\n",
    "    if parallel:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    priority_matcher = SimilarityPipeline(\n",
    "        matcher=CosineSimilarity(),\n",
    "        extractor=DeepFeatures(model=model, device=device, batch_size=batch_size, num_workers=0),\n",
    "        transform=T.Compose([T.Resize([384, 384]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    )\n",
    "    return priority_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Kornia_Extractor:\n",
    "    def __init__(self, model_name: str, mx_keypoints: int = 1000, device: None | str = None, num_workers: int = 0, only_xy: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (str): 'KeyNetAffNetHardNet', 'HesAffNetHardNet', 'KeyNetHardNet', 'GFTTAffNetHardNet', 'SIFT', 'DISK'.\n",
    "            mx_keypoints (int, optional): maximum number of points for a Descriptor.\n",
    "            device (str, optional): Device used for inference. Defaults to None.\n",
    "            only_xy(bool, optional): \n",
    "                If False, returns the affine transformation.\n",
    "                If True, returns the coordinates.\n",
    "            num_workers(int, optional): how many subprocesses to use for data loading.\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if model_name == 'KeyNetAffNetHardNet':\n",
    "            model = KF.KeyNetAffNetHardNet(mx_keypoints, True)\n",
    "        elif model_name == 'HesAffNetHardNet':\n",
    "            model = KF.HesAffNetHardNet(mx_keypoints, True)\n",
    "        elif model_name == 'KeyNetHardNet':\n",
    "            model = KF.KeyNetHardNet(mx_keypoints, True)\n",
    "        elif model_name == 'GFTTAffNetHardNet':\n",
    "            model = KF.GFTTAffNetHardNet(mx_keypoints, True)\n",
    "        elif model_name == 'SIFT':\n",
    "            model = KF.SIFTFeature(mx_keypoints, True)\n",
    "        elif model_name == 'DISK':\n",
    "            model = KF.DISK.from_pretrained(\"depth\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "            \n",
    "        self.model = model.eval()\n",
    "        self.device = device\n",
    "        self.num_workers = num_workers\n",
    "        self.only_xy = only_xy\n",
    "        self.mx_keypoints = mx_keypoints\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, dataset: ImageDataset) -> FeatureDataset:\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        features = []\n",
    "        self.model.to(self.device)\n",
    "        for image, _ in tqdm(loader, mininterval=1, ncols=100):\n",
    "            if self.model_name == 'DISK':\n",
    "                img = image.to(self.device)\n",
    "                with torch.inference_mode():\n",
    "                    output = self.model(img, n=self.mx_keypoints)[0]\n",
    "                    kpt = output.keypoints.cpu()\n",
    "                    kpt_laf = KF.laf_from_center_scale_ori(\n",
    "                        kpt.unsqueeze(0),\n",
    "                        torch.ones(1, len(kpt), 1, 1, device='cpu'))\n",
    "                    \n",
    "                    output = {\n",
    "                        'keypoints': kpt_laf.squeeze(0).cpu(),  # [N, 2, 3]\n",
    "                        'keypoint_scores': output.detection_scores.cpu(),\n",
    "                        'descriptors': output.descriptors.cpu(),\n",
    "                        'image_size': torch.tensor(image.shape[2:])\n",
    "                    }\n",
    "            else:\n",
    "                img = K.color.rgb_to_grayscale(image).to(self.device)\n",
    "                with torch.inference_mode():\n",
    "                    output = self.model(img)\n",
    "                    output = {\n",
    "                        'keypoints': output[0].squeeze(0).cpu(),\n",
    "                        'keypoint_scores': output[1].squeeze(0).cpu(),\n",
    "                        'descriptors': output[2].squeeze(0).cpu(),\n",
    "                        'image_size': torch.tensor(image.shape[2:])\n",
    "                    }\n",
    "            \n",
    "            # Дополнение ключевых точек до max_keypoints\n",
    "            if output['keypoints'].shape[0] < self.mx_keypoints:\n",
    "                pad_size = self.mx_keypoints - output['keypoints'].shape[0]\n",
    "                output['keypoints'] = torch.cat([\n",
    "                    output['keypoints'],\n",
    "                    torch.zeros(pad_size, 2, 3, device='cpu')\n",
    "                ], dim=0)\n",
    "                output['keypoint_scores'] = torch.cat([\n",
    "                    output['keypoint_scores'],\n",
    "                    torch.zeros(pad_size, device='cpu')\n",
    "                ], dim=0)\n",
    "                output['descriptors'] = torch.cat([\n",
    "                    output['descriptors'],\n",
    "                    torch.zeros(pad_size, output['descriptors'].shape[1], device='cpu')\n",
    "                ], dim=0)\n",
    "            \n",
    "            if self.only_xy:\n",
    "                output['keypoints'] = KF.get_laf_center(output['keypoints'].unsqueeze(0)).reshape(-1, 2) + 0.5\n",
    "            features.append(output)\n",
    "        self.model.to(\"cpu\")\n",
    "        \n",
    "        return FeatureDataset(\n",
    "            metadata=dataset.metadata,\n",
    "            features=features,\n",
    "            col_label=dataset.col_label,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Kornia_Matcher(MatchPairs):\n",
    "    def __init__(self, init_threshold: float = 0.1, device: str | None = None,\n",
    "                 matcher_name='adalam', extract=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "        self.init_threshold = init_threshold\n",
    "        self.matcher_name = matcher_name\n",
    "        self.extract = extract\n",
    "        \n",
    "        self.model = None\n",
    "        if self.matcher_name == 'LightGlueMatcher':\n",
    "            name = {\n",
    "                'DISK': 'disk', \n",
    "                'SIFT': 'sift',\n",
    "                'KeyNetAffNetHardNet': 'keynet_affnet_hardnet'\n",
    "            }\n",
    "            self.model_name = name[extract]\n",
    "    \n",
    "    def _load_model(self):\n",
    "        if self.model is None and self.matcher_name == 'LightGlueMatcher':\n",
    "            self.model = KF.LightGlueMatcher(self.model_name).eval().to(self.device)\n",
    "    \n",
    "    def get_matches(self, batch):\n",
    "        self._load_model()\n",
    "        idx0, data0, idx1, data1 = batch\n",
    "        all_scores, all_matches = [], []\n",
    "        \n",
    "        try:\n",
    "            for i in range(data1['image_size'].shape[0]):\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                if len(data0[\"keypoints\"][i]) == 0 or len(data1[\"keypoints\"][i]) == 0:\n",
    "                    all_scores.append(torch.empty(0))\n",
    "                    all_matches.append(torch.empty(0, 2, dtype=torch.long))\n",
    "                    continue\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    desc1 = data0[\"descriptors\"][i].to(self.device)\n",
    "                    desc2 = data1[\"descriptors\"][i].to(self.device)\n",
    "                    lafs1 = data0[\"keypoints\"][i].unsqueeze(0).to(self.device)\n",
    "                    lafs2 = data1[\"keypoints\"][i].unsqueeze(0).to(self.device)\n",
    "                    hw1 = data0['image_size'][i]\n",
    "                    hw2 = data1['image_size'][i]\n",
    "                    \n",
    "                    if self.matcher_name == 'adalam':\n",
    "                        scores_, matches_ = KF.match_adalam(\n",
    "                            desc1=desc1, desc2=desc2,\n",
    "                            lafs1=lafs1, lafs2=lafs2,\n",
    "                            hw1=hw1, hw2=hw2,\n",
    "                        )\n",
    "                    elif self.matcher_name == 'fginn':\n",
    "                        scores_, matches_ = KF.match_fginn(\n",
    "                            desc1=desc1, desc2=desc2,\n",
    "                            lafs1=lafs1, lafs2=lafs2,\n",
    "                        )\n",
    "                    elif self.matcher_name == 'GADMatcher':\n",
    "                        scores_, matches_ = KF.GeometryAwareDescriptorMatcher(match_mode='fginn')(\n",
    "                            desc1=desc1, desc2=desc2,\n",
    "                            lafs1=lafs1, lafs2=lafs2,\n",
    "                        )\n",
    "                    elif self.matcher_name == 'LightGlueMatcher':\n",
    "                        scores_, matches_ = self.model(\n",
    "                            desc1=desc1, desc2=desc2,\n",
    "                            lafs1=lafs1, lafs2=lafs2,\n",
    "                            hw1=hw1, hw2=hw2,\n",
    "                        )\n",
    "                    \n",
    "                    if matches_.shape[0] == 0:\n",
    "                        all_scores.append(torch.empty(0))\n",
    "                        all_matches.append(torch.empty(0, 2, dtype=torch.long))\n",
    "                    else:\n",
    "                        all_scores.append(scores_.detach().cpu())\n",
    "                        all_matches.append(matches_.long().detach().cpu())\n",
    "                    \n",
    "                    del desc1, desc2, lafs1, lafs2, scores_, matches_\n",
    "                \n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        data = []\n",
    "        for i, (i0, i1, scores, matches) in enumerate(zip(idx0, idx1, all_scores, all_matches)):\n",
    "            if len(matches) == 0 or len(data0[\"keypoints\"][i]) == 0 or len(data1[\"keypoints\"][i]) == 0:\n",
    "                data.append({\n",
    "                    \"idx0\": i0.item(),\n",
    "                    \"idx1\": i1.item(),\n",
    "                    \"kpts0\": np.empty((0, 2)),\n",
    "                    \"kpts1\": np.empty((0, 2)),\n",
    "                    \"scores\": np.empty(0),\n",
    "                })\n",
    "            else:\n",
    "                matches_long = matches.long()\n",
    "                data.append({\n",
    "                    \"idx0\": i0.item(),\n",
    "                    \"idx1\": i1.item(),\n",
    "                    \"kpts0\": data0[\"keypoints\"][i][matches_long[:, 0]].cpu().numpy(),\n",
    "                    \"kpts1\": data1[\"keypoints\"][i][matches_long[:, 1]].cpu().numpy(),\n",
    "                    \"scores\": scores.numpy(),\n",
    "                })\n",
    "        return data\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'model') and self.model is not None:\n",
    "            del self.model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def get_preds_by_treshold(coefs, scores, dataset0, dataset1):\n",
    "    coefs = np.array(coefs)\n",
    "    coefs_expanded = coefs[:, np.newaxis, np.newaxis]  # Теперь форма (3, 1, 1)\n",
    "    similarity = np.mean(np.array(scores)*coefs_expanded, axis=0)\n",
    "    similarity = np.where(np.isnan(similarity), 0, similarity)\n",
    "    test_only_idx = list(set(dataset0.metadata[\"identity\"]) - set(dataset1.metadata[\"identity\"]))\n",
    "    predictions, scores_model = get_preds(dataset1.labels_string, similarity, len(dataset0))\n",
    "    best = {'geo_mean': -1}\n",
    "    for threshold in np.arange(0.02, 0.98, 0.02):\n",
    "        pred = predictions.copy()\n",
    "        pred[scores_model < threshold] = 'new_individual'\n",
    "        baks = BAKS(dataset0.labels_string, pred, test_only_idx)\n",
    "        baus = BAUS(dataset0.labels_string, pred, test_only_idx, \"new_individual\")\n",
    "        geo_mean = np.sqrt(baks * baus)\n",
    "        best['geo_mean'] = max(best['geo_mean'], geo_mean)\n",
    "    return 1 - best['geo_mean']\n",
    "\n",
    "class WildFusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        calibrated_pipelines: list[SimilarityPipeline],\n",
    "        priority_pipeline: SimilarityPipeline | None = None,\n",
    "    ):\n",
    "        self.calibrated_pipelines = calibrated_pipelines\n",
    "        self.priority_pipeline = priority_pipeline\n",
    "        self.coefs = None\n",
    "\n",
    "    def fit_calibration(self, dataset0: ImageDataset, dataset1: ImageDataset):\n",
    "        for matcher in self.calibrated_pipelines:\n",
    "            matcher.fit_calibration(dataset0, dataset1)\n",
    "        if (self.priority_pipeline is not None) and (self.priority_pipeline.calibration is not None):\n",
    "            self.priority_pipeline.fit_calibration(dataset0, dataset1)\n",
    "\n",
    "    def get_optim(self, dataset0: ImageDataset, dataset1: ImageDataset, B=25):\n",
    "        if B is not None:\n",
    "            pairs = self.get_priority_pairs(dataset0, dataset1, B=B)\n",
    "        scores = []\n",
    "        for matcher in self.calibrated_pipelines:\n",
    "            scores.append(matcher(dataset0, dataset1, pairs=pairs))\n",
    "        scores = np.where(np.isnan(np.array(scores)), 0, np.array(scores))\n",
    "        res = minimize(get_preds_by_treshold, ([0.5 for _ in range(len(scores))]), (scores, dataset0, dataset1), method='Powell')\n",
    "        print(res.x, res.fun)\n",
    "        self.coefs = res.x\n",
    "        coefs_expanded = self.coefs[:, np.newaxis, np.newaxis]\n",
    "        similarity = np.mean(np.array(scores)*coefs_expanded, axis=0)\n",
    "        return similarity\n",
    "    \n",
    "    def get_priority_pairs(self, dataset0: ImageDataset, dataset1: ImageDataset, B: int) -> np.ndarray:\n",
    "        if self.priority_pipeline is None:\n",
    "            raise ValueError(\"Priority matcher is not assigned.\")\n",
    "        priority = self.priority_pipeline(dataset0, dataset1)\n",
    "        _, idx1 = torch.topk(torch.tensor(priority), min(B, priority.shape[1]))\n",
    "        idx0 = np.indices(idx1.numpy().shape)[0]\n",
    "        grid_indices = np.stack([idx0.flatten(), idx1.flatten()]).T\n",
    "        return grid_indices\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        dataset0: ImageDataset,\n",
    "        dataset1: ImageDataset,\n",
    "        pairs: list | None = None,\n",
    "        B: int = None,\n",
    "    ):\n",
    "        if B is not None:\n",
    "            pairs = self.get_priority_pairs(dataset0, dataset1, B=B)\n",
    "        scores = []\n",
    "        for matcher in self.calibrated_pipelines:\n",
    "            scores.append(matcher(dataset0, dataset1, pairs=pairs))\n",
    "        scores = np.where(np.isnan(np.array(scores)), 0, np.array(scores))\n",
    "        coefs_expanded = self.coefs[:, np.newaxis, np.newaxis]\n",
    "        similarity = np.mean(np.array(scores)*coefs_expanded, axis=0)\n",
    "        return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Conveyor\n",
    "class Conveyor:\n",
    "    def __init__(self, query, database, device='cpu'):\n",
    "        self.Test = query\n",
    "        self.Train = database\n",
    "        self.device = device\n",
    "\n",
    "    def build_model(self, priority_matcher, local_matchers):\n",
    "        self.wildfusion = WildFusion(priority_pipeline=priority_matcher, calibrated_pipelines=local_matchers)\n",
    "    def similarity(self, BBB=25):\n",
    "        self.sim = self.wildfusion(self.Test, self.Train, B=BBB)\n",
    "    def predict(self, save_path = None):\n",
    "        predictions, scores = get_preds(self.Train.labels_string, self.sim, len(self.Test))\n",
    "        if save_path:\n",
    "            sub = pd.DataFrame({\n",
    "                'image_id': self.Test.df['image_id'],\n",
    "                'identity': predictions,\n",
    "                'scores': scores\n",
    "            })\n",
    "            sub.to_csv(save_path, index=False)\n",
    "        return predictions, scores\n",
    "    def calibration(self, data):\n",
    "        database, query = data\n",
    "        self.wildfusion.fit_calibration(query, database)\n",
    "    def get_best_thresholds(self, data, BBB=25):\n",
    "        database, query = data\n",
    "        similarity = self.wildfusion(query, database, B=BBB)\n",
    "        test_only_idx = list(set(query.metadata[\"identity\"]) - set(database.metadata[\"identity\"]))\n",
    "        predictions, scores_model = get_preds(database.labels_string, similarity, len(query))\n",
    "\n",
    "        best = {'geo_mean': -1, 'threshold': None}\n",
    "        scores = []\n",
    "\n",
    "        for threshold in tqdm(np.arange(0.001, 0.9, 0.001)):\n",
    "            pred = predictions.copy()\n",
    "            pred[scores_model < threshold] = 'new_individual'\n",
    "            baks = BAKS(query.labels_string, pred, test_only_idx)\n",
    "            baus = BAUS(query.labels_string, pred, test_only_idx, \"new_individual\")\n",
    "            geo_mean = np.sqrt(baks * baus)\n",
    "            if geo_mean > best['geo_mean']:\n",
    "                best['geo_mean'] = geo_mean\n",
    "                best['threshold'] = threshold\n",
    "            scores.append([threshold, baks, baus, geo_mean])\n",
    "        self.best_thresholds = best['threshold']\n",
    "        return best['threshold'], best['geo_mean'], scores\n",
    "    \n",
    "    def plot_scores(self, scores, graf_name='nothink'):\n",
    "        thresholds = [i[0] for i in scores]\n",
    "        baks_scores = [i[1] for i in scores]\n",
    "        baus_scores = [i[2] for i in scores]\n",
    "        geo = [i[3] for i in scores]\n",
    "        plt.plot(thresholds, baks_scores, label='BAKS (Known)')\n",
    "        plt.plot(thresholds, baus_scores, label='BAUS (Unknown)')\n",
    "        plt.plot(thresholds, geo, label='Geometrical Mean')\n",
    "        plt.title(graf_name)\n",
    "        plt.xlabel('thresholds')\n",
    "        plt.ylabel('metrics')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# root = './animal-clef-2025'\n",
    "root = '/kaggle/input/animal-clef-2025'\n",
    "threshold = 0.6\n",
    "\n",
    "transform_display = T.Compose([T.Resize([384, 384])])\n",
    "transform = T.Compose([T.Resize([512, 512]), T.ToTensor()])\n",
    "models = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = AnimalCLEF2025(root, transform=transform_display, load_label=True)\n",
    "df = dataset.df\n",
    "train = df[df['split'] == 'database']\n",
    "test = df[df['split'] == 'query']\n",
    "df.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seg = pd.read_csv('/kaggle/input/data/segmentation_salamanders.csv') # path to segmentation by yolo\n",
    "seg['segmentation']=seg['segmentation'].apply(lambda x: eval(x))\n",
    "seg['bbox']=seg['bbox'].apply(lambda x: np.array(x))\n",
    "dataset_with_seg = AnimalCLEF2025(root, img_load='bbox_mask', load_label=True)\n",
    "dataset_with_seg.df['image_name'] = dataset_with_seg.df['path'].apply(lambda x: x.split('/')[-1])\n",
    "dataset_with_seg.df = dataset_with_seg.df.merge(seg, on='image_name', how='left')\n",
    "df_seg = dataset_with_seg.df\n",
    "train_seg = df_seg[df_seg['split'] == 'database']\n",
    "test_seg = df_seg[df_seg['split'] == 'query']\n",
    "df_seg.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('conservationxlabs/miewid-msv3', trust_remote_code=True).eval()\n",
    "priority_matcher = SimilarityPipeline(\n",
    "    matcher=CosineSimilarity(),\n",
    "    extractor=DeepFeatures(model, device=device, batch_size=64, num_workers=0),\n",
    "    transform=T.Compose([T.Resize((440, 440)),T.ToTensor(),T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    calibration=IsotonicCalibration()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_matchers = [\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='superpoint', device=device),\n",
    "        extractor=SuperPointExtractor(device=device, max_num_keypoints=512),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='aliked', device=device),\n",
    "        extractor=AlikedExtractor(device=device, max_num_keypoints=512),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='disk', device=device),\n",
    "        extractor=DiskExtractor(device=device, max_num_keypoints=512),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='superpoint', device=device),\n",
    "        extractor=SuperPointExtractor(device=device, max_num_keypoints=1024),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='aliked', device=device),\n",
    "        extractor=AlikedExtractor(device=device, max_num_keypoints=1024),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=MatchLightGlue(features='disk', device=device),\n",
    "        extractor=DiskExtractor(device=device, max_num_keypoints=1024),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kornia_matchers = [\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='GADMatcher', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='DISK', mx_keypoints=512, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='adalam', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='DISK', mx_keypoints=512, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='GADMatcher', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='KeyNetAffNetHardNet', mx_keypoints=512, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='adalam', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='KeyNetAffNetHardNet', mx_keypoints=512, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([512, 512]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='GADMatcher', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='DISK', mx_keypoints=1024, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([1024, 1024]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='adalam', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='DISK', mx_keypoints=1024, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([1024, 1024]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='GADMatcher', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='KeyNetAffNetHardNet', mx_keypoints=1024, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([1024, 1024]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "    SimilarityPipeline(\n",
    "        matcher=Kornia_Matcher(matcher_name='adalam', device=device, collector = CollectCounts(thresholds=[0.0])),\n",
    "        extractor=Kornia_Extractor(model_name='KeyNetAffNetHardNet', mx_keypoints=1024, device=device, only_xy=False),\n",
    "        transform=T.Compose([T.Resize([1024, 1024]), T.ToTensor()]),\n",
    "        calibration=IsotonicCalibration()\n",
    "    ),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "local_matchers.append(SimilarityPipeline(\n",
    "    matcher=CosineSimilarity(),\n",
    "    extractor=DeepFeatures(model, device=device, batch_size=64, num_workers=0),\n",
    "    transform=T.Compose([T.Resize((440, 440)),T.ToTensor(),T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),\n",
    "    calibration=IsotonicCalibration()\n",
    "))\n",
    "model2 = timm.create_model('hf-hub:BVRA/wildlife-mega-L-384', num_classes=0, pretrained=True, device=device)\n",
    "local_matchers.append(SimilarityPipeline(\n",
    "    matcher=CosineSimilarity(),\n",
    "    extractor=DeepFeatures(model=model2, device=device, batch_size=64, num_workers=0),\n",
    "    transform=T.Compose([T.Resize([384, 384]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]),\n",
    "    calibration=IsotonicCalibration()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "all_data.append({\n",
    "    'name': 'SalamanderID2025',\n",
    "    'train': dataset_with_seg.get_subset(train_seg.loc[train_seg['dataset'] == 'SalamanderID2025'].index),\n",
    "    'test': dataset_with_seg.get_subset(test_seg.loc[test_seg['dataset'] == 'SalamanderID2025'].index),\n",
    "    'threshold_df': split_data(dataset_with_seg, train_seg.loc[train_seg['dataset'] == 'SalamanderID2025']),\n",
    "    'calibration_df': split_data(dataset_with_seg, train_seg.loc[train_seg['dataset'] == 'SalamanderID2025'][:300])\n",
    "})\n",
    "all_data.append({\n",
    "    'name': 'LynxID2025',\n",
    "    'train': dataset.get_subset(train.loc[train['dataset'] == 'LynxID2025'].index),\n",
    "    'test': dataset.get_subset(test.loc[test['dataset'] == 'LynxID2025'].index),\n",
    "    'threshold_df': split_data(dataset, train.loc[train['dataset'] == 'LynxID2025']),\n",
    "    'calibration_df': split_data(dataset, train.loc[train['dataset'] == 'LynxID2025'][:300])\n",
    "})\n",
    "all_data.append({\n",
    "    'name': 'SeaTurtleID2022',\n",
    "    'train': dataset.get_subset(train.loc[train['dataset'] == 'SeaTurtleID2022'].index),\n",
    "    'test': dataset.get_subset(test.loc[test['dataset'] == 'SeaTurtleID2022'].index),\n",
    "    'threshold_df': split_data(dataset, train.loc[train['dataset'] == 'SeaTurtleID2022']),\n",
    "    'calibration_df': split_data(dataset, train.loc[train['dataset'] == 'SeaTurtleID2022'][:300])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for data in all_data:\n",
    "    print(data['name'])\n",
    "    model = Conveyor(query=data['test'], database=data['train'], device=device)\n",
    "    model.build_model(priority_matcher=priority_matcher, local_matchers= kornia_matchers + local_matchers)\n",
    "    print('calibration')\n",
    "    model.calibration(data['calibration_df'])\n",
    "\n",
    "    print('get best threshold')\n",
    "    threshold, geo_mean, metrics = model.get_best_thresholds(data['threshold_df'], BBB=50)\n",
    "    \n",
    "    print(f\"best threshold: {round(threshold, 4)}, with geo_mean: {round(geo_mean, 4)}\")\n",
    "    model.plot_scores(scores=metrics, graf_name=f\"Train metrics on {data['name']}\")\n",
    "    \n",
    "    print('similarity')\n",
    "    model.similarity(BBB=50)\n",
    "    predictions, scores = model.predict(save_path=f'submission_{data[\"name\"]}.csv')\n",
    "    \n",
    "    models.append({\n",
    "        'model_name': data['name'],\n",
    "        'model': model,\n",
    "        'data_predictions': predictions,\n",
    "        'data_scores': scores,\n",
    "        'data_threshold': threshold,\n",
    "    })\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dat in models:\n",
    "    ans = dat['data_predictions'].copy()\n",
    "    ans[dat['data_scores'] < dat['data_threshold']] = 'new_individual'\n",
    "    test.loc[test['dataset'] == dat['model_name'], 'identity'] = ans\n",
    "print(test.shape)\n",
    "test.value_counts('identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\n",
    "    'image_id': test['image_id'],\n",
    "    'identity': test['identity']\n",
    "})\n",
    "sub.to_csv('sample_submission.csv', index=False)\n",
    "# sub.to_csv('./sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11223220,
     "sourceId": 91451,
     "sourceType": "competition"
    },
    {
     "datasetId": 7367925,
     "sourceId": 11736487,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7052044,
     "sourceId": 11739244,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7385279,
     "sourceId": 11763900,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
